---
title: "读书笔记《大语言模型》"
showSummary: true
summary: "《大语言模型》的阅读笔记，相对于《大规模语言模型》，这本更偏向于相关的理论结合实践和应用。"
layoutBackgroundBlur: true
layoutBackgroundHeaderSpace: false
date: 2024-06-30
tags: ["算法原理", "阅读笔记"]
---
{{< katex >}}

{{< alert "tag">}}
本文主要是对[《大语言模型》](https://llmbook-zh.github.io/)的阅读笔记
{{< /alert >}}

## 基础概念

### 语言模型的发展

![任务求解能力.png](image%2F%E4%BB%BB%E5%8A%A1%E6%B1%82%E8%A7%A3%E8%83%BD%E5%8A%9B.png)

#### **统计语言模型 (Statistical Language Model, SLM)**

基于马尔科夫假说(Markov Assumption) 建立语言序列的预测模型，通常是根据一个固定长度 \\(n\\) 的前缀预测下一个目标单词，这种模型通常被称为 \\(n\text{\textendash}gram\\) 语言模型。但这种模型通常会产生“**维数灾难**” (Curse of Dimensionality)，虽说可以通过平滑策略来缓解数据稀疏的问题，但是这又导致无法建模复杂的语义关系。

#### **神经语言模型 (Neural Language Model, NLM)**

通过神经网络来建模文本序列的生成。最早可以追溯到分布式词表示 (Distributed Word Representation) 这一概念，即基于聚合上下文特征（分布式词向量）构建目标词的预测模型，分布式词向量又称为”**词嵌入**“ (Word Embedding)。这种基于隐式语义特征表示的语言模型构建方法为自然语言处理提供了一种较为通用的解决途径。

分布式词表示使用了**低维稠密向量**来表示词汇的语义，这与**基于词典空间的稀疏向量**表示(One-Hot Representation) 有本质不同，能够刻画更丰富的隐含语义特征，转换后的向量示例如下：

![One-Hot&&Embedding.png](image%2FOne-Hot%26%26Embedding.png)

#### **预训练模型 (Pre-trained Language Model, PLM)**

与上面的词嵌入模型相比，预训练语言模型在**训练架构**和**训练数据**两个方面进行了改进和创新。[ELMo](https://arxiv.org/abs/1802.05365) 是一个早期有代表性的预训练语言模型，提出了使用大量无标注数据训练双向 LSTM (Bidirectional LSTM, biLSTM) 网络，预训练完成后所得到的 biLSTM 可以用来学习上下文感知的单词表示，进一步 ELMo 可以根据下游任务数据对 biLSTM 网络进行微调 (Fine-Tuning)。

但是传统神经网络的长文本建模能力较弱且不容易并行训练。2017年谷歌提出的基于自注意力机制 (Self-Attention) 的 Transformer 模型 (***[Attention is All you Need.[2017-01]](https://arxiv.org/abs/1706.03762)***)，这个模型是可以通过 GPU 或 TPU 进行加速训练。基于 Transformer 和预训练架构，谷歌提出了预训练模型 [BERT](https://arxiv.org/abs/1810.04805) 。

同一时期，OpenAI 提出了基于 Transformer 仅有解码器的 [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) 模型，以及基于下一个词元预测的预训练任务进行训练。

{{< alert "lightbulb">}}
一般来说，**编码器架构**被认为更适合解决**自然语言理解任务** 如完形填空等；**解码器架构**更适合解决**自然语言生成任务** 如文本摘要等。因为编码器通常一次性输入整个序列，并生成固定长度的内部表示；但解码器通常是一步步生成输出，每一步生成依赖之前的输出。
{{< /alert >}}

#### **大语言模型 (Large Language Model, LLM)**

研究人员发现，在小型预训练模型的基础上，通过**扩展法则** (Scaling Law) 会使下游任务模型的性能提升。一些研究工作尝试训练更大的预训练语言模型如175B参数的 GPT-3 和540B参数的PaLM，相比小型预训练模型如330M参数的 BERT 和1.5B 参数的 GPT-2，这种大模型可以通过”**上下文学习”** (In-Context Learning, ICL) 使用少量样本数据解决下游问题，并且还拥有小模型所不具备的”**涌现能力**” (Emergent Abilities)。为了和预训练模型作区分，学术界将这些大型预训练模型命名为”大语言模型”。

### 扩展法则 (Scaling Law)

大模型成功的关键正是在于“规模扩展” (Scaling) 的利用。**扩展法则** (Scaling Law) 对研究大模型规模扩展所带来的模型性能提升具有重要的实践指导意义。下面主要介绍 2 种扩展法则：

#### **KM 扩展法则**

> ***参考论文 [Scaling Laws for Neural Language Models.[2020-01]](https://arxiv.org/abs/2001.08361)***
> 

2020年OpenAI 团队首次建立了神经语言模型性能与三个主要因素——模型规则 (N)、数据规模 (D) 和 计算算力 (C) 之间的幂律关系 (Power-Law Ralationship)。这个关系是通过模型在不同数据规模 (22M 到 23B 词元)、模型规模 (768M 到 1.5B 非嵌入参数)和算力规模下的性能表现拟合推到得到的。具体公式如下：

在给定算力预算 \\(c\\) 的条件下，可以近似得到以下三个基本指数公式来描述扩展法则：

$$
L(N) = (\frac{N\_c}{N})^{\alpha\_N}, \space\alpha\_N \thicksim 0.076,\space N\_c \thicksim 8.8 \times10^{13}\\\\L(D) = (\frac{D\_c}{D})^\alpha\_D, \space \alpha\_N \thicksim 0.095,\space D\_c\thicksim 5.4\times 10^{13}\\\\ L(C) = (\frac{C\_c}{C})^{\alpha\_C}, \space \alpha\_C \thicksim 0.050, \space C\_c \thicksim 3.1 \times 10^{8}
$$

其中 \\(L(\sdot)\\) 表示以 \\(nat\\) (以 \\(e\\)  为底信息量的自然对数) 为单位的交叉熵损失，\\(N_c \space D_c \space C_c\\) 是实验性的常数数值，分别对应于非嵌入参数数量、训练数据量和实际的算力开销。

为了便于理解扩展法则，OpenAI 研究团队又将损失函数进一步分解为两部分：不可约损失 (真实数据分布的熵) 和 可约损失(真实分布和模型分布之间 KL 散度的估计)：

$$
L(x) = \underbrace{L\_\infty}\_{不可约损失} + \underbrace{(\frac{x\_0}{x})^{\alpha\_x}}\_{可约损失}
$$

这里的 \\(x\\) 是一个占位符，可以只带上面的\\(N 、D 、 C\\)。这个公式表示

- 不可约损失由数据自身特征确定，无法通过扩展法则或者优化算法进行约减
- 模型性能的优化只能通过减少可约损失部分

#### **Chinchilla 扩展法则**

> ***参考论文 [Training Compute-Optimal Large Language Models.[2020-03]](https://arxiv.org/abs/2203.15556)***
> 

2022年由 DeepMind 团队提出的一种可选的扩展法则，旨在充分利用算力资源对大模型进行优化训练。此法则主要是针对更大范围的模型规模 (70M 到 16B 参数) 和数据规模 (5B 到 500B 词元)进行实验，并拟合得到了另一种关于模型性能的幂律关系，公式如下：

$$
L(N,D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta } \\\\ \\\\其中 \space  E=1.69,\space A=406.4, \space B=410.7, \space \alpha =0.34, \space \beta=0.28
$$

进一步，利用约束条件 \\(C \approx 6ND\\) 对损失函数 \\(L(N,D)\\) 进行推导，能够得到算力资源固定情况下模型规模与数据规模的最有分配方案：

$$
N_{opt}(C) = G(\frac{C}{6})^a \space, \space a = \frac{\alpha}{\alpha+\beta}\\ D_{opt}(C) = G^{-1}(\frac{C}{6})^b\space, \space b= \frac{\beta}{\alpha+\beta}
$$

其中 \\(G\\) 是由 \\(A, B,\alpha,\beta\\) 计算得出的扩展系数。

再进一步，研究人员发现 KM 扩展法则和 Chinchilla 扩展法则都可以近似表示成以算力为核心的公式：

$$
N_{opt} \approx C^a\space , \space D_{opt} \approx C^b
$$

通过上面的推演能够知道，当算力\\(C\\) 给定的情况下，最有的模型参数规模和数据规模由指数系数 \\(a\\) 和 \\(b\\) 分别确定：

- 当 \\(a > b\\)  时，应该用更多算力去提高模型的参数规模
- 当 \\(a < b\\) 时，应该用更多算力去提高数据规模
- 当 \\(a \approx b\\)  时，两种规模参数应该以等比例关系增加

KM 扩展法则 (\\(a \approx 0.73, b\approx 0.7\\)) 倾向于将更大的算力分配给模型规模的增加；Chinchilla 扩展法测 (\\(a\approx 0.46, b \approx 0.54\\)) 倾向于两种规模参数以等比例关系增加；但越来越多的工作表明，现有预训练模型对于数据的需求量远高于扩展法则中给出的估计规模，如 LLaMA2-7B 的模型使用了 2T 的词元进行训练，很多更小的模型可能够通过超大规模的预训练数据获得比较大的模型性能提升。这种现象的原因主要在于 Transformer 架构具有较好的数据扩展性。到目前为止还没有实验能够有效验证扩展到多大的数据规模后，模型的性能将不再提升。

#### 讨论

- 可预测的扩展 (Predictable Scaling)：扩展法则可以用于指导大语言模型的训练，通过较小的算力资源可靠地估计较大算力资源投入后的模型性能，这被称为**可预测的扩展**。这种可预测新主要体现在两个方面：① 使用小模型性能预估大模型性能 ② 使用大模型早期的预训练性能估计训练完成后的性能。同时随着模型规模的增大，可供训练的数据量会变得“枯竭”，如何在数据首先情况下建立扩展法则，也具有实践意义。
- 任务层面的可预测性：可约损失的减少可以提高模型整体的性能，但是并不代表模型在执行下游任务的性能改善。对于某些任务甚至会出现“逆向扩展”，即模型损失降低任务性能却变差。目前通过扩展法则仅能准确预测某些任务如编码能力，但对其他能力如上下文学习、涌现能力仍然是不可预测的。

### 涌现

在论文 ***[Emergent Abilities of Large Language Models.[2022-06]](https://arxiv.org/abs/2206.07682)*** 中，定义了在大语言模型中出现的涌现能力。指的是当模型扩展到一定规模，模型的特定任务性能突然会出现显著跃升的趋势，远超随机水平。

涌现能力可以定义为解决某些复杂任务的能力水平，但作者在这里更关注可以用来解决任何问题的普适能力。具体有三种典型的涌现能力：

- 上下文学习 (In-context Learning, ICL)：指的是在 prompt 提示中为语言模型提供自然语言指令和多个任务示例，无需显式训练或梯度更近，仅输入文本就能为测试样本生成预期输出。(***参考自论文 [Language Models are Few-Shot Learners.[2020-05]](https://arxiv.org/abs/2005.14165)***)
- 指令遵循 (Instruction Following)：指大语言模型能欧按照自然语言指令来执行对应的任务。为了获得这种能力，通常需要使用自然语言模型的示例数据进行微调，称为指令微调 (Instruction Tuning) 或监督微调 (Supervised Fine-tuning)。通过指令微调，大语言模型可以在没有显式示例的情况下，按照任务指令完成新的任务，有效提升模型的泛化能力。 (***参考自论文 [Training language models to follow instructions with human feedback.[2022-03]](https://arxiv.org/abs/2203.02155)、[Finetuned Language Models Are Zero-Shot Learners.[2021-09]](https://arxiv.org/abs/2109.01652)、[Multitask Prompted Training Enables Zero-Shot Task Generalization.[2021-10]](https://arxiv.org/abs/2110.08207)***)
- 逐步推理 (Step-by-step Reasoning)：大语言模型可以利用思维链 (Chain-of-Thought, CoT) 提示策略来加强推理性能，具体来说可以在提示中引入任务相关的中间推理步骤来加强负责任务的求解，从而获得可靠的答案。 (***参考自论文 [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.[2022-01]](https://arxiv.org/abs/2201.11903)***)
    
    与扩展法测通过语言建模算是衡量语言模型的整体性能不同，涌现能力通常使用任务性能来衡量模型性能，整体上展现出随规模扩展的骤然跃升趋势，具有不可预测性。
    
    但也有观点认为，涌现能力可能部分归因于 ① 现有评测集中通常采用不连续的评估指标 ② 较为有限的模型参数规模，这两个情况很容易导致下游任务的评测效果产生不连续的变化趋势，从而导致所谓的“涌现”。
    
    目前还缺少对于大语言模型涌现机制的基础性解释研究工作。与这一问题较为相关的研究叫做 “顿悟” (Grokking)，指的是训练过程中的一种数据学习模式，模型性能从随机水平提升为高度泛化。(***参考自论文 [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.[2022-01]](https://arxiv.org/abs/2201.02177)***)
    

### 大模型的发展与衍生

![GPT系列模型技术发展历程.png](image%2FGPT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B.png)


![LLaMA 系列模型的衍生模型.png](image%2FLLaMA%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A1%8D%E7%94%9F%E6%A8%A1%E5%9E%8B.png)


## 预训练

### 数据准备

#### 预处理

数据处理主要包含以下几个步骤。原始语料库可以直接看原文，这里就不赘述了。

![数据准备.png](image%2F%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87.png)

对于数据预处理，主要分为三部分：

1. **质量过滤**
    1. **基于启发式规则的方法**：这种方法主要有**基于语种的过滤**、**基于简单统计的过滤**和**基于关键词的过滤**。基于简单统计的过滤是使用语料中标点符号的分布和比率、句子的长度等特征来衡量文本质量，也可以基于困惑度等文本生成的评估指标来衡量，以此过滤低质量的数据。基于关键词就不用详细说明了，这种方式的优点在于更加精准。
    2. **基于分类器的方法**：选取部分代表性的数据进行质量标注，以此训练一个精准的文本质量分类器。目前常用来实现分类器的方法包括轻量级模型如 FastText 等、可微调的预训练语言模型如 BERT、BART 或 LLaMA 等，以及闭源大模型 API 如 GPT-4、Claude3。
2. **敏感内容过滤**
    1. **过滤有毒内容**：[Jigsaw 评论数据集](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge)提供了用于训练毒性分类器的数据，包括”有毒”、“严重有毒”、“有威胁”、“侮辱性”、“暴力”以及“身份仇恨”六个类别。利用这一数据集进行训练可以构建出高效的毒性文本分类器。并通过设置合理的阈值，训练好的分类器可以有效识别并过滤含有有毒内容的信息。
    2. **过滤隐私内容**：预训练文本数据大多来自互联网，其中可能包括用户生成的敏感信息或可识别的个人信息 (Personally Identifiable Information, PII)。 使用启发式方法比如关键词识别来检测和删除私人信息是一种直接且有效的方法。 
3. **数据去重**：由于大模型有较强的数据拟合和记忆能力，很容易导致模型过度学习，此外也可以导致训练过程不稳定等情况。去重算法的设计通常是基于不同的计算粒度以及匹配方法。
    1. **计算粒度**：去重可以在**句子级别**、**文档级别**和**数据集级别**等多种粒度上进行。
    2. **用于去重的匹配方法**：有**精确匹配算法**和**近似匹配算法**两种。对于精确匹配来说，通常使用后缀数组来匹配最小长度的完全相同子串；对于近似匹配来说，通常采用局部敏感哈希 (Locality-Sensitive Hashing, LSH) 算法。

#### 词元化 (分词)

词元化 (Tokenization) 是数据预处理中的一个关键步骤，旨在将原始文本分割成模型可识别和建模的词元序列。子词分词器 (Subword Tokenizer) 被广泛应用于基于 Transformer 的语言模型中，包括 BPE 分词、WordPiece 分词和 Unigram 分词三种常见方法。

**BPE 分词**

BPE 算法在1994年被提出，最早应用于通用的数据压缩 (***参考论文 [A New Algorithm for Data Compression.[1994-02]](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)***)，自然语言处理领域的研究人员将其适配被应用于文本分词 (***参考论文 [Neural Machine Translation of Rare Words with Subword Units.[2016-06]](https://arxiv.org/abs/1508.07909)***)。

BFE算法从一组基本符号如字母或边界字符开始，迭代寻找语料库中两个相邻词元，并将它们替换为新的词元，这一过程被称为**合并**。合并的时候会计算两个连续词元的共现频率，即每次迭代中最频繁出现的一对词元会被选择与合并，一直持续到预定义的词表大小。

BPE算法还有一种拓展算法——字节集别的BPE (Byte-level BPE, B-BPE)，它将字节视为合并操作的基本符号，从而实现更细粒度的分割。GPT-2、BART 和 LLaMA 都是采用的这种词元化方法。

{{< alert "-">}}
假设语料中包含了五个英文单词：<br/>
**”loop”, “pool”, “loot”, “tool”, “loots”**<br/>
这种情况下，BPE 的初始词汇表是 ：<br/>
**[”l”, “o”, “p”, “t", “s”]**<br/>
实践中的基础词汇表可以包含所有 ASCII 字符和 一些 Unicode 字符 (如中文的汉字)，语料库没有的字符会被转换为未知词元如 “<UNK>”。<br/>

假设单词在语料库中的频率如下：<br/>
**(”loop”, 15), (”pool”, 10), (”loot”, 10), (”tool”, 5), (”loots”, 8)**<br/>
其中出现频率最高的是 “oo”，出现了 48 次，因此学习到的第一条合并规则是 (”o”, “o”) → “oo”，并被添加到词汇表中。在这一次合并后，词汇和语料库如下：<br/>
词汇：**[”l”,  “o”, “p”, “t”, “s”, “oo”]**<br/>
语料库：**(”l”, “oo”, “p”, 15), (”p”, “oo”, “l”, 10), (”l”, “oo”, “t”, 10), (”t”, “oo”, “l”, 5), (”l”, “oo”, “t”, “s”, 8)**<br/>
此时出现频率最高的配对是(”l”, “oo”)，因此第二条合并规则是 (”l”, “oo”) → “loo”。将其添加到词汇表中并合并后，词汇和语料库如下：<br/>
词汇：**[”l”,  “o”, “p”, “t”, “s”, “oo”, ”loo”]**<br/>
语料库：**(”loo”, “p”, 15), (”p”, “ool”, 10), (”loo”, “t”, 10), (”t”, “oo”, “l”, 5), (”loo”, “t”, “s”, 8)**<br/>
重复上述过程，直到达到所设置的终止词汇量。<br/>

{{< /alert >}}

**WordPiece 分词**

WordPiece 是谷歌内部非公开的分词算法，最初在开发语言搜索系统时被提出 (***参考论文 [Japanese and korean voice search.[2012-03]](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/37842.pdf)***)；随后在2016年被用于机器翻译系统，并于2018年被 BERT 采用作为分词器。

与 BPE 类似，WordPiece 分词算法也是从一个小的词汇表开始，其中包括模型使用的特殊词元和初始词汇表。由于它是通过添加前缀来识别子词的，因此每个词的初始拆分都是将前缀添加到词内的所有字符上。如 “word” 会被拆分为 “w##o##r##d”。同时 WordPiece 分词算法并不是选择最频繁的词对，而是使用下面的公式为每个词对计算分数：

$$
得分 = \frac{词对出现的频率}{第一个词出现的频率 \times 第二个词出现的频率}
$$

**Unigram 分词**

与前两个分词算法不同，Unigram 分词方法从语料库的一组足够大的字符串和词元初始集合开始，通过期望最大化 (Expectation-Maximization, EM) 算法计算词元的概率并计算损失值 lossi，根据损失值排序并保留对整体概率 \\(L\\) 影响大的词元，直到达到预期的词表大小 (***参考论文 [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates.[2018-04]](https://arxiv.org/abs/1804.10959)***)。

**分词器的选择**

`SentencePiece` 代码库支持了字节级别的 BPE 分词和 Unigram 分词。

为了训练出高效的分词器，需要重点关注一下几个因素：

- 分词器必须剧本无损重构的特征，即分词结果能够准确无误地还原为原始输入文本
- 分词器应具有高压缩率，即在给点文本数据的情况下，经过分词处理后的词元数量应该尽可能少。压缩率的计算公式如下
    
    $$
    压缩率=\frac{UTF{-}8 字节数}{词元数}
    $$
    

此外还需要注意，在处理中文等非英语数据时，分词器的表现可能不佳；以及为了提高某些特定能力如数学，可能需要针对性地设计分词器。

### 模型架构

这部分不会再详细介绍传统 Transformer 的架构和 MoE 架构的具体细节，想获得详细内容的可以直接阅读原文。

![Transformer 模型架构.png](image%2FTransformer%20%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.png)


![混合专家模型.png](image%2F%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B.png)

#### 主流架构

- **编码器-解码器架构**：这是经典的模型结构，原始的Transformer 模型使用的此架构，即组合了两个分别担任编码器和解码器的 Transformer 模块。此架构在编码器端采用双向自注意力机制对输入信息进行编码处理，并在解码器端使用交叉注意力与掩码自注意力机制通过自回归来生成输出。
- **因果解码器架构**：目前绝大部分主流的大语言模型采用了因果解码器架构。该架构采用了单向的掩码注意力机制，并没有显式区分输入和输出部分，使得每个输入的词元只关注序列中位于它前面的词元和它本身，进而自回归得预测输出词元。此外由于不含有编码器部分，因果解码器无需关注编码器表示的交叉注意力模块，经过自注意力模块后的词元表示将直接送入到前馈神经网络中。
- **前缀解码器架构**：前缀解码器架构也被称为非因果解码器架构。前缀解码器对于输入 (前缀) 部分使用双向注意力进行编码，而对于输出部分利用单向的掩码注意力利用该词元前面和自身进行自回归预测。前缀解码器在编码和解码过程中是共享参数的，并没有划分为独立的解码器和编码器。

![编码器和解码器.png](image%2F%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E8%A7%A3%E7%A0%81%E5%99%A8.png)

#### 长上下文模型

目前增强大语言模型长文本建模能力的研究主要集中在两个方向：

- 扩展位置编码：在处理更长的文本时，RoPE 在每个子空间需要处理更大的旋转角度，但旋转角度可能会超过训练中的角度分布范围。因此很多研究旨在提升其不经过训练或继续训练情况下对长文本的建模能力。
- 调整上下文窗口：采用受限的注意力机制来调整原始的上下文窗口，从而实现对更长文本的有效建模，进一步可以理解为通过忽略部分不重要的词元从而扩展上下文。主要有三种调整方法：并行上下文窗口、Λ 形上下文窗口和词元选择，如下图。其中白色表示被掩盖的次元，蓝色表示进行注意力计算的词元，块上的数字表示位置编码的相对位置。

   ![长上下文模型.png](image%2F%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E6%A8%A1%E5%9E%8B.png) 

## 微调与对齐

### 指令微调

指令微调 (Instruction Tuning) 是指使用自然语言形式的数据对预训练后的大模型进行参数微调。在另外一些参考文件中，也被称为有监督微调 (Supervised Fine-tuning) 或多任务提示训练 (Multitask Prompted Training)。 (***参考论文 [Finetuned Language Models Are Zero-Shot Learners.[2021-09]](https://arxiv.org/abs/2109.01652)、[Training language models to follow instructions with human feedback.[2022-03]](https://arxiv.org/abs/2203.02155)、[Multitask Prompted Training Enables Zero-Shot Task Generalization.[2021-10]](https://arxiv.org/abs/2110.08207)***)

指令微调旨主要在三个方面对大语言模型产生影响：① 整体任务性能 ② 任务求解能力增强 ③ 领域专业化适配

参数高效微调 (Parameter-efficient Fine-tuning)，也称为轻量化微调 (Lightweight Fine-tuning)，旨在减少需要训练的模型参数量，同时保证微调后的模型性能能够与全量微调的表现媲美。下面介绍几种微调方法，其中比较主流的是 LoRA 微调。

#### 低秩适配微调方法 (LoRA 微调)

研究人员发现模型在针对特定任务进行适配时，参数矩阵往往是过参数化 (Over-parameterized) 的，存在一个较低的内在秩。LoRA 提出在预训练模型的参数矩阵上添加低秩分解矩阵来近似每层的参数更新，从而减少适配下游任务所需要训练的参数。(***参考论文 [LoRA: Low-Rank Adaptation of Large Language Models.[2021-06]](https://arxiv.org/abs/2106.09685)***)

![低秩适配微调方法.png](image%2F%E4%BD%8E%E7%A7%A9%E9%80%82%E9%85%8D%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.png)

给定一个参数矩阵 \\(W\\)，其更新过程可以一般性地表达为以下形式：

$$
W=W_0 + \Delta W, W_0 \in \Reals^{H\times H} 
$$

其中 \\(W_0\\) 是原始参数矩阵，\\(\Delta W\\) 是更新的梯度矩阵。LoRA 的基本思想是冻结原始矩阵 \\(W_0\\)，通过低秩分解矩阵来近似参数更新矩阵，其中 \\(R\\)  是减小后的秩且远小于 \\(H\\)：

$$
\Delta W = A \cdot B^T, \space A \in \Reals^{H \times R}, \space B\in \Reals^{H \times R} \\ W = W_0 + A \cdot B^T
$$

除此之外 LoRA 还有很多变种

- AdaLoRA：引入了动态低秩适应，解决了原始LoRA 中每个低秩参数 \\(R\\) 都被设置为固定且相同数值的情况。具体来说模型在微调过程中通过损失来衡量每个参数矩阵对训练结果的重要性，重要性较高的矩阵被赋予比较高的秩，相反不太重要的参数矩阵被赋予比较低的秩。
- QLoRA：QLoRA 将原始的参数矩阵量化为 4 比特，而低秩参数部分仍随时用 16 比特进行训练，在保持微调效果的同时进一步节省显存的开销。

#### 适配器微调 (Adapter Tuning)

适配器微调 (Adapter Tuning) 在 Transformer 模型中引入了小型神经网络模块，被称为适配器。为了实现适配器模块，研究者提出使用瓶颈网络架构：首先将原始的特征向量压缩到较低维度，然后使用激活函数进行非线性变化，最后将其恢复到原始维度。(***参考论文 [Parameter-Efficient Transfer Learning for NLP.[2019-06]](https://arxiv.org/abs/1902.00751)***)

![适配器微调.png](image%2F%E9%80%82%E9%85%8D%E5%99%A8%E5%BE%AE%E8%B0%83.png)

用公式表示如下，其中 \\(R\\)  是减小后的秩且远小于 \\(H\\) ：

$$
h = h + \sigma(h \cdot W^d) \cdot W^u, \space W^d \in \Reals^{H \times R}, \space W^u \in \Reals^{R \times H} 
$$

#### 前缀微调 (Prefix Tuning)

前缀微调 (Prefix Tuning) 在语言模型的每个多头注意力层中都添加了一组前缀参数。这些前缀参数组成了一个可训练的连续矩阵，可以视为若干虚拟词元的嵌入向量。 (***参考论文 [Prefix-Tuning: Optimizing Continuous Prompts for Generation.[2021-01]](https://arxiv.org/abs/2101.00190)***)。

![前缀微调.png](image%2F%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83.png)

基于原始的注意力计算公式，前缀词元被拼接到每个注意力的键向量与值向量之前，计算公式可以表示如下：

$$
原始注意力：\\\\head_n=Attention(XW^Q_n, XW^K_n, XW^V_n) \\\\ 前缀拼接后：\\\\head = Attention(XW^Q, P^K \oplus XW^K,P^V\oplus XW^V) \\ P^K, P^V \in \Reals^{L \times H}
$$

其中 \\(\oplus\\) 表示矩阵拼接；\\(L\\) 代表前缀向量的长度，一般在 10 到 100 之间，可以根据任务场景进行调整。

#### 提示微调

提示微调仅在输入嵌入层中加入可训练的提示向量。

P-tuning 提出了使用自由形式来组合输入文本和提示向量，通过LSTM 来学习软提示词元的表示，他可以同时适用于自然语言理解和生成任务 (***参考论文 [GPT Understands, Too.[2023-10]](https://arxiv.org/abs/2103.10385)***)。

Prompt Tuning 以前缀形式添加提示词元，直接在输入前拼接连续性向量。 (***参考论文 [The Power of Scale for Parameter-Efficient Prompt Tuning.[2021-09]](https://arxiv.org/abs/2104.08691)***)。

![提示微调.png](image%2F%E6%8F%90%E7%A4%BA%E5%BE%AE%E8%B0%83.png)

### 人类对齐

在大语言模型的学习过程中，如何确保大语言模型的行为与人类的价值观、真实意图和社会伦理相一致成为了一个关键研究问题，通常这一研究问题被称为**人类对齐** (Human Alignment)。

人类对齐主要有三个具有代表性的3H对齐标准，分别是：

- **有用性 (Helpfulness)**：大模型需要提供有用的信息，能够准确完成任务、正确理解上下文，并展现出一定的创造性与多样性。模型应尽量以简洁、高效的方式协助用户完成任务。当任务描述存在歧义或者涉及背景信息时，模型应具备主动询问并获取任务相关信息的能力，同时具有一定的敏感度、洞察力和审慎态度。
- **诚实性 (Honesty)**：模型的输出应具备真实性和客观性，不应夸大或歪曲事实，避免产生误导性陈述，并能够应对输入的多样性和复杂性。在人机交互过程中，大模型应用应向用户提供准确内容，还应适当表达对于输出信息的不确定性程度，以避免任何形式的误导。本质上，这要求模型了解自身的能力和知识水平。诚实性是一个更为客观的标准，对人类标注的依赖相对较少。
- **无害性 (Harmlessness)**：大语言模型应避免生成肯呢个引发潜在负面影响或威海的内容。在处理敏感主题时，模型应遵循道德标准和社会价值，从而消除冒犯性与歧视性。此外模型需要能够检测且具有恶意目的的查询请求。当模型被诱导执行危险行为时，应直接予以拒绝。

以上三种通用的对齐标注较为宽泛，因此许多研究针对性地给出了一些更为细化的对齐标准，例如：

- 行为对齐要求人工智能系统能够做出符合人类期望的行为
- 意图对齐进一步要求大语言模型在意图和行为上都要与人类期望保持一致
- 道德对齐要求语言模型应避免涉及非法、不道德或有害的话题，在回应中优先考虑用户安全、道德准绳和行为边界

已有的对齐标准主要是基于人类认知进行设计的，具有一定的主观性。因此直接通过优化目标来建模这些对齐标准较为困难。接下来，介绍一下具体的对齐方法。

#### 基于人类反馈的强化学习 (RLHF)

基于人类反馈的强化学习 (Reinforcement Learning fro Human Feedback, RLHF) 旨在利用收集到的人类反馈数据指导大语言模型进行微调，从而使得大语言模型在多个标准上与人类对齐。

![RLHF.png](image%2FRLHF.png)

RLHF 整体训练框架有三个阶段：

1. **监督微调**
    
    为了让待对齐语言模型具有较好的指令遵循能力，通常需要收集高质量的指令数据进行微调。指令数据可以由人类标注员编写也可以由大语言模型自动生成。
    
2. **奖励模型训练**
    
    使用语言模型针对任务指令生成一定数量的候选输出。随后标注员对输出文本进行偏好标注，如根据话标注进行打分或根据自身偏好对输出进行全排序。进一步使用人工标注的偏好数据进行奖励模型的训练，建模人类偏好。
    
3. **强化学习训练**
    
    待对齐模型接收提示作为输入并输出文本；奖励模型则根据当前语言模型的输出提供相应的奖励分数，用于指导策略模型的优化。为了避免当前训练轮次的语言模型明显偏离初始模型 (即强化学习之前的预训练模型)，通常会在原始优化目标中加一个惩罚项如KL散度。KL 散度越大，意味着当前语言模型越偏离初始语言模型。**PPO 算法**目前是被一种广泛用于人类对齐的强化学习算法。
    

近端策略优化 (Proximal Policy Optimization, PPO) 算法用于训练能够根据外部环境做出行为决策的策略模型。PPO 算法的训练流程如下：

{{< alert "-">}}
输入：\\(SFT\\) 模型 \\(SFT_\theta\\) ，奖励模型 <br/>
输出：与人类偏好对其的大语言模型 \\(\pi_{\theta}\\)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初始化负责与环境交互的策略模型：\\(\pi_{\theta old} \gets SFT_{\theta}\\)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初始化负责学习的策略模型：\\(\pi_\theta \gets SFT_{\theta}\\)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\(\bold{for} \space step = 1,2 ...\space \bold{do}\\)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\(\pi_{\theta old}\\) 采样得到若干角色轨迹 \\(\{\tau_1, \tau_2 \}\\)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据**优势函数** \\(\hat{A}_t\\) 计算”优势估计”<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\(\bold{for} \space j=1,2,...,K \space \bold{do}\\)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据**梯度剪裁 \\(J\_{\tiny{CLIP}}(\theta)\\)** 或**KL散度 \\(J\_{\tiny{KLPEN}}(\theta)\\)**  计算目标函数<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用梯度上升优化 \\(\pi\_{\theta}\\) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\(\bold{end \space for}\\)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更新与环境交互的策略模型：\\(\pi\_{\theta old} \gets \pi\_{\theta}\\)<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\(\bold{end \space for}\\)<br/>

{{< /alert >}}

#### 非强化学习的对齐方法

RLHF 在训练过程中需要维护和更新多个模型，不仅占用大量的内存资源，而且整个算法的执行过程也比较复杂。同时 PPO 算法在优化过程中的稳定性和难度比较高。为了克服这些问题，研究人员提出了直接基于监督微调的对其方法，旨在通过更简洁、更直接的方式来实现大语言模型与人类价值观的对齐。其中比较有代表性的算法是 DPO 算法。

DPO (Direct Preference Optimization) 算法的主要思想是在强化学习的目标函数中建立决策函数与奖励函数之间的关系，以规避奖励模型的过程。大概的步骤如下：

1. 找到奖励函数 \\(r(x,y)\\) 与决策函数 \\(\pi_{\theta}(y|x)\\) 之间的关系
2. 通过奖励模型的方法来直接建立训练目标和决策函数 \\(\pi_{\theta}(y|x)\\) 之间的关系

#### SFT 和 RLHF 优缺点

SFT 优点：

1. 提高大语言模型在各种基准测试中的性能
2. 增强大语言模型在不同人物上的繁花能力
3. 提升大语言模型在专业领域的能力

SFT 缺点：

1. 当数据超出大语言模型的知识范围时，模型易产生幻觉
2. 通过对教师教师模型的蒸馏，会增加学生模型出现幻觉的可能性
3. 不同标注者对示例数据标注的差异，会影响 SFT 的学习性能
4. 指令数据的质量会影响大语言模型的训练效果

RLHF 优点：

1. 进一步增强模型的能力，提高模型有用性
2. 有效减轻大语言模型出现有害响应的可能性
3. 有效减轻大语言模型出现幻觉的可能性
4. 偏好标注可以减轻示例生成过程中的不一致情况

RLHF 缺点：

1. 训练样本使用效率较低
2. 训练过程不稳定，训练过程对超参数敏感
3. 依赖强大的 SFT 模型进行热启动

## 大模型的使用

### 解码与部署

#### 解码策略

大模型的生成方式本质上是一个概率采样过程，需要合适的解码策略来生成合适的输出内容。这里主要介绍自回归场景下的解码策略：模型 \\(M\\) 每次根据当前上下文词元序列 \\(\bold{u}=[u_1, u_2,...,u_t]\\) 建模下一个词的概率分布 \\(P\\)，然后根据一定的解码策略选择下一个词 \\(u'\\)，之后再将 \\(u\\) 和 \\(u'\\) 作为新的上下文重复上述步骤，直到生成结束词元或达到长度上限为止。

这里提到的解码策略主要有两种：

- 贪心搜索 (Greedy Search)：在每个生成步骤中都选择概率最高的词元。其公式如下
    
    $$
    u_i=\argmax_u P(u|\bold{u}_{<i})
    $$
    
    贪心搜索提供了一些参数对其进行了一些改进，如：
    
    - 束搜索 (Beam Search)：在解码过程中保留前 n 个具有最高概率的句子，并最终选取整体概率最高的生成回复。这里的 n 被称为束大小。但 \\(n = 1\\) 时，束搜索就退化为贪心搜索。
    - 长度惩罚 (Length Penalty)：束搜索中需要比较不同长度候选句子的概率，往往需要引入长度惩罚，或称为长度归一化。传统束搜索会倾向于生成较的句子，因此在生成概率的计算中引入长度惩罚，从而鼓励模型生成更长的句子。
    - 重复惩罚：为了缓解贪心搜索重复生成的问题，如出现惩罚 (Presence Penalty) 和频率惩罚 (Frequency Penalty)。出现惩罚在生成过程中会将已经生成词元的 \\(logits\\) 减去惩罚项 \\(\alpha\\) 来降低该词元之后生成的概率。频率惩罚会记录每个词元生成的数目，然后减去出现次数乘以惩罚项 \\(\alpha\\)。
- 随机采样 (Probability Sampling)：根据模型建模的概率分布采样得到下一个词元，旨在增强生成过程中的随机性和结果多样性。其公式如下：
    
    $$
    u_i \text{\textasciitilde}P(u|\bold{u_{<i}})
    $$
    
    随机采样也提供了一些参数来改进策略，如
    
    - 温度采样 (Temperatur Sampling)：为了调节采样过程中的随机性，一种比较有效的方法是调整 \\(softmax\\) 函数中的温度系数。降低温度系数 \\(t\\) 会使得概率分布更加集中；当温度系数 \\(t\\) 为 1 时，改公式退化为标准的随机采样方法；当温度系数 \\(t\\) 为 0 时，等同于贪心搜索。
    - \\(Top\text{\textendash}k\\) 采样 (Top-k Sampling)：top-k 采样策略是直接剔除概率较低的词元，限制模型从概率最高的前 \\(k\\) 个词元进行采样。
    - \\(Top\text{\textendash}p\\) 采样 (Top-p Sampling)：top-p 采样又称为核采样 (Nucleus Sampling)，核心思想是从一个符合特定概率条件的最小词元集合中进行采样，要求其中包含的所有词元累计概率大于或等于预设阈值 \\(p\\)。
    - 对比解码 (Contrastive Decoding)：大模型相较于小模型更倾向于为重要词元分配更高的概率。基于这个想法，对比解码通过计算一对较大和较小的语言模型之间的对数概率分布差，然后基于归一化的差值分布采样下一个词元，从而有效提升重要词元在生成过程中的影响力。

#### 模型量化 (Model Quantization)

在神经网络压缩中，量化指的是从浮点数到整数的映射过程，目前比较常用的是 8 比特整数量化，即 **INT8** 量化。神经网络中通常由两种类型的数据需要量化，分别是**权重量化** (也称为模型参数量化) 和**激活 (值) 量化**。

量化的过程可以表示为一个函数，该函数将连续的输入映射到离散的输出集合，这个过程涉及到四舍五入或截断等近似操作。一个一般形式的量化函数如下，其中 \\(S\\) 表示缩放因子，用于确定裁剪范围；\\(Z\\) 表示零点因子，用于确定对称或非对称量化；\\(R\\) 表示将缩放后的浮点数映射为近似整数的取整操作：

$$
x_q = R(x /S)-Z
$$

下面介绍一些函数量化常见的分类与实现策略：

- **均匀量化和非均匀量化**：根据映射函数的数值范围是否均匀，可以将量化分类为均匀量化和非均匀量化。均匀量化是指在量化过程中，量化函数产生的量化值之间的间距是均匀分布的，这种方法通常用于将连续的数据转换成离散表示。与此相对，非均匀量化方法的量化值不一定均匀分布，可以根据输入数据的分布范围进行调整。
- **对称量化和非对称量化**：根据零点因子 \\(Z\\) 是否为 0，均匀量进一步可以分为两类：对称量化 (\\(Z=0\\)) 和非对称量化 (\\(Z \not = 0\\)) ，关键的区别在于整数区间零点的映射。
- **量化粒度的选择**：量化算法通常针对一个批次的数据进行处理，其中批次的规模大小就反映了量化粒度。

**权重量化**

主流的权重量化方法通常是基于逐层量化的方法进行设计的，旨在通过最小化逐层的重构损失来优化模型的量化权重。公式如下，其中\\(W\\)，\\(\hat{W}\\) 分别表示原始权重量化后的权重，\\(X\\) 为输入：

$$
argmin_{\hat{W}}||XW-\hat{X}W||^2_2
$$

为了优化目标函数，GPTQ 的基本思想是在逐层量化的基础上，进一步将权重矩阵按照维度分组，例如 128 列为一组，对一个组内逐列进行量化，每列参数量化后调整组内其他为量化的参数。GPTQ 还进一步采用了特殊设计的优化方法来加速整个量化过程。 (***参考论文 [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.[2023-03]](https://arxiv.org/abs/2210.17323)***)

AWQ 发现在逐层和逐组权重量化过程中，对于模型性能重要的权重只占全部参数的一小部分 (0.1%~1%)，更应该关注那些与较大激活值维度对应的关键权重。基于此 AWQ 方法提出引入针对权重的激活感知缩放策略，使得量化更加针对处理关键权重所对应的权重维度。(***参考论文 [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.[2024-04]](https://arxiv.org/abs/2306.00978)***)

**权重和激活值量化**

- **细粒度量化**：对于 Transformer 模型来说，权重和激活值通常以张量形式表示。可以使用粗粒度的方法量化，对每一个张量使用一整套量化参数；也可以使用更为细粒度的方法来减少量化误差。
- **混合精度分解**：当模型参数规模超过一定阈值(如6.7B)后，神经网络中的激活值会出现一些异常的超大数值，称为异常值涌现现象。 基于这一发现，在矩阵乘法中，可以将具有异常值的特征维度与其他正常维度分开计算。
- **量化难度平衡**：在模型量化过程中，由于激活值中的异常值问题比权重更加明显，导致激活值往往比权重更难量化。

#### 模型蒸馏 (Model Distillation)

模型蒸馏的目标是将复杂模型 (称为教师模型) 包含的只是迁移到简单模型 (称为学习模型) 中，从而实现复杂模型的压缩。一般来讲会使用教师模型的输出来训练学生模型，以此传递模型知识。模型蒸馏的核心思想是，引入额外的损失函数 (称为蒸馏随时函数)，训练学生模型的输出尽可能接近教师模型输出，

根据蒸馏知识的不同，传统模型蒸馏方法分为两种类型：

- **基于反馈的知识蒸馏**：该干法主要关注教师模型最后一层输出的 \\(logits\\)，这些 \\(logits\\) 经过 \\(softmax\\) 变换后，可以用作学生模型的 “软标签” 来进行学习。优化的核心目标是让学生模型输出的 \\(logits\\) 去近似教师模型输出的 \\(logits\\)。
- **基于特征的知识蒸馏**：基于中间特征表示的蒸馏关注教师模型的中间层输出的激活值，并使用这些激活值作为监督信息训练学生模型。中间层特征提供了更为丰富的模型信息，有助于在模型蒸馏过程中实现更有效的知识迁移。

#### 模型剪枝 (Model Pruning)

模型剪枝的目标是在尽可能不损失模型性能的情况下，努力消减模型的参数数量，最终有效降低模型的显存需求以及算力开销。

传统模型剪枝方法一般可以分为两类：

- **结构化剪枝 (Structured Pruning)**：旨在去除对性能影响较小的模型组件，可以删除神经元、通道甚至中间层。结构化剪枝的核心思想是，在尽量保持模型预测精度的条件下，去除那些对于结果影响不大的结构单元，如注意力机制中的注意力头、前馈层权重中的特定维度等。那么如何判断需要去除那些呢？具体来说可以计算每一维列向量权重的数值大小作为判断其重要性的标准，然后以此去除重要性较低的维度。
- **非结构化剪枝 (Unstructured Pruning)**：主要关注去除模型权重矩阵中不重要的数值。非结构化剪枝通过创建一个包含 0/1 的掩码矩阵，并将这一矩阵与原始权重相乘，其中 0 所在位置的权重不会在计算中产生作用。剪枝完成后，那些被剪枝掉的位置只会存储数值 0，从而节省了存储空间。

### 提示学习

#### 要素与原则

针对大语言模型的提示设计需要考虑四个关键要素：

- **任务描述**：这部分展示了大语言模型应当遵循的具体指令
- **输入数据**：用户可以直接使用自然语言描述输入数据的内容。对于特殊形式的输入数据，需要采用合适的方式使其能够被大语言模型读取与理解。
- **上下文信息**：引入的外部信息，可以通过合适的格式化，作为大模型的上下文，以加强大语言模型对他们的利用。
- **提示策略**：针对不同大语言模型设计合适的提示策略对于激发模型解决塔顶任务的能力非常重要。在某些情况下，添加特定的前缀或后缀有助于引导大语言模型解决复杂任务。

**设计原则**

- 清晰地表达任务目标：一个清晰详尽的任务描述中应当包含任务的各种要素信息，如任务目标、输入/输出数据和回复限制。
- 分解为简单且详细的子任务：该原则的目标是将一个复杂任务分解为若干相对独立但又相互关联的子任务，每个任务都对应原始任务的某个方面或步骤。
- 提供少样本示例：少样本示例有助于大语言模型在无需调整参数的前提下学习输入与输出之间的语义映射关系
- 采用模型友好的提示格式：大语言模型采用了专门构建的数据集进行预训练，因此可以从数据集中学习到大量的语言表达模式，发现并利用这些语言表达模式可以帮助我们更有效地使用大语言模型完成特定任务。

#### 上下文学习 （In-context learning, ICL）

在 GPT-3 的论文中，OpenAI 研究团队首次提出上下文学习。上下文学习使用任务描述和 (或) 示例所组成的自然语言文本提示。上下文学习的提示构建过程如下：

$$
\underbrace{LLM}\_{大语言模型} (\underbrace{I}\_{任务描述}, \underbrace{f(x\_1,y\_1),...,f(x\_k,y\_k)}\_{示例},{f(\underbrace{x\_{k+1}}\_{输入},\underbrace{\text{\textunderscore}\text{\textunderscore}}_{答案})})\to \hat{y}\_{k+1}
$$


如上图，首先通过自然语言描述，并从任务数据集中选择一些样本作为示例。其次根据特定的模板，将这些示例按照特定顺序组合成提示内容。最后，将测试样本添加到提示后面，整体输入到大语言模型以生成输出。

上下文学习本身不难理解，但是他的内在原理和工作机制值得深究。文中给出了两个核心影响点：

- **预训练对上下文学习能力的影响**
    - **预训练任务**：GPT-3的论文通过相关实验发现上下文学习能力随着模型规模的增大而增强。后续研究又发现，即便是小模型，通过设计专门的训练任务进行预训练或微调，也能够获得上下文学习能力。这表明训练任务的设计对于上下文学习能力的习得有重要影响。(***论文参考 [Pre-Training to Learn in Context.[2023-05]](https://arxiv.org/abs/2305.09137)、[MetaICL: Learning to Learn In Context.[2022-05]](https://arxiv.org/abs/2110.15943)***)
    - **预训练数据**：研究发现，通过混合不同领域的预训练数据，增强预训练语料的多样性可以提高大语言模型的上下文学习能力。此通过前后相关的文本直接拼接进行训练，模型能够更好地理解文本之间的关联性，从而提升上下文学习能力。(***论文参考 [On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model.[2022-05]](https://arxiv.org/abs/2204.13509)、[In-context Pretraining: Language Modeling Beyond Document Boundaries.[2023-10]](https://arxiv.org/abs/2310.10638)***)
- **推理生成阶段大模型执行上下文学习的方式**
    - **任务识别**：基于概率近似正确 (Probably Approximately Correct, PAC) 的理论框架认为预训练数据中存在能够表征各种任务信息的隐变量。因此在上下文学习中，大模型具备从给定示例中学习并编码这些隐变量的能力，以此实现上下文学习的自动识别和适应。(***论文参考 [The Learnability of In-Context Learning.[2023-03]](https://arxiv.org/abs/2303.07895)***)
    - **任务学习**：从隐式梯度下降的角度分析，上下文学习机制可以被分解为以下两个步骤，首先大模型通过前向计算过程，针对给定示例生成相应的元梯度，然后模型利用注意力机制隐式地执行了梯度下降。这个过程类似传统机器学习中的参数更新。(***论文参考 [Transformers learn in-context by gradient descent.[2022-12]](https://arxiv.org/abs/2212.07677)、[Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers.[2022-12]](https://arxiv.org/abs/2212.10559)***)

#### 思维链提示

思维链提示作为上下文学习的一种扩展形式，将原始的<输入, 输出>映射关系转换为<输入, 思维链, 输出>的三元组形式。在思维链提示的作用下，大语言模型可以根据输入生成对应的思维链及答案。

同样地，这里描述一下思维链推理能力的内在原理和工作机制：

- 思维链推理能力的来源
    - 斯坦福大学的研究人员假设思维链对大语言模型有效的原因是训练数据中存在很多相互重叠且相互影响的局部变量空间。在这个条件下，即使两个变量没有在训练数据中共现，也可以通过一系列重叠的中间变量的推理而联系起来。(***论文参考 [Why think step by step? Reasoning emerges from the locality of experience.[2023-04]](https://arxiv.org/abs/2304.03843)***)
    - 除此之外还有从函数学习的角度出发，认为复杂推理任务可以看做是一种组合函数，因此思维链推理实际上是将组合函数的学习过程分解为了两个不同阶段：① 信息聚焦 ②上下文单步的组合函数。(***论文参考 [Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning.[2023-11]](https://arxiv.org/abs/2305.18869)***)
- 思维链提示对模型推理的影响：研究人员发现具体的提示内容并不重要，比如思维链示例中算式正确与否对模型的性能影响很小；重要的是它们与问题相关性以及推理过程的逻辑性。进一步，也有研究发现，即使不对模型使用思维链提示，只要其生成的文本中包含显示的推理过程，也能显著改善模型的推理能力。(***论文参考 [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters.[2022-12]](https://arxiv.org/abs/2212.10001)、[Chain-of-Thought Reasoning Without Prompting.[2024-02]](https://arxiv.org/abs/2402.10200)***)

### 智能体 Agent

智能体是一个具备环境感知、决策指定及动作执行能力的自主算法系统。旨在模拟人类或其他生物的智能行为，自动化地解决问题或执行任务。(***论文参考 [More Agents Is All You Need.[2024-02]](https://arxiv.org/abs/2402.05120)***)

#### 基础组件

下面主要介绍基于大语言模型的智能体的构建过程，主要围绕三个基本组件进行：

- **记忆组件 (Memory)**：主要用于存储智能体与环境的历史交互记录，并能够随时检索使用，这些形式可以是文本形式，也可以是图像、声音等多模态形式。记忆可以继续细分为下面两种
    - **短期记忆**：负责暂时存储和处理智能体 相关信息的记忆载体。短期记忆通常对应于模型内部的上下文窗口，即输入窗口。短期记忆中的信息存储时间相对短，并且对于信息容量有一定的限制。
    - **长期记忆**：长期记忆是智能体存储长期累积信息的记忆载体。长期记忆单元中的存储内容具有持久性，涵盖事实知识、基础概念、过往经验以及重要技能等多个层面的信息。大语言模型通过检索机制读取长期记忆中的信息，并借助反思机制进行信息的写入与更新 (***论文参考 [Reflexion: Language Agents with Verbal Reinforcement Learning.[2023-03]](https://arxiv.org/abs/2303.11366)***)。
- **规划组件 (Planning)**：规划组件为智能体引入了类似人类解决任务的思考方式，将复杂任务分解为一系列简单的子任务，进而逐一进行解决。这种方法有助于提高问题解决的效率和效果，提高了智能体对复杂环境的适应性和操作的可靠性。
- **执行组件 (Execution)**：主要用于执行由规划组件指定的任务解决方案。通过设置执行组件，智能体可以产生具体的动作行为，进而与环境进行交互，并获得实际的执行效果反馈。具体来讲，智能体会在执行决策的过程中执行规划组件指定的明确行动规划，同时会参考记忆组件中的长短期记忆来帮助执行准确的行动。在技术实现上，执行组件可以通过语言模型自身来完成预定规划，或者通过继承外部工具来增强其执行能力。(***论文参考 [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.[2023-05]](https://arxiv.org/abs/2305.04091)、[ReAct: Synergizing Reasoning and Acting in Language Models.[2022-10]](https://arxiv.org/abs/2210.03629)***)

#### 多智能体系统的构建

与单智能体系统的独立工作模式不同，在多智能体系统中，每个智能体扮演特定角色并承担对应的个功能，着重强调智能体之间的协同合作，以发挥集体智慧的优势。下面介绍 2 种多智能体系统的通讯协同机制：

- **多智能体系统的构建方法**：智能体的主要任务可以被设定为规划设计、获取新知识、对事物或现象进行评判等类型，每个智能体应具备独特的功能特征、层次结构以及行为策略，进而实现多智能体系统整体的目标。其次比较关键的是定义多智能体之间的交互方式如协作、竞争、信息交流等方面，以及制定协议、策略、博弈论规则，以此确保智能体之间能够有效协同。除此之外还需要考虑系统的可扩展性、可维护性、安全性以及智能体之间的异构性等。
- **多智能体系统的通讯协同机制**：多智能体系统中，通讯机制和协同机制是实现智能体之间有效协作的重要基础技术。下面介绍这两种重要机制
    - **通讯机制**：多智能体体统的通讯机制通常包括三个基本要素：① 通讯协议 ② 通讯拓扑 ③ 通讯内容。通讯协议规定了智能体之间如何进行信息交换和共享；通讯拓扑定义了智能体之间的连接关系；通讯内容则是指智能体之间实际传输的信息。
    - **协同机制**：多智能体系统的协同机制通常包括协作、竞争和协商。协作指的是智能体通过共享资源、信息和任务分配来实现共同目标；竞争主要涉及到在资源有限的环境中，智能体之间的竞争；协商指的是通过交换信息和让步来解决目标或资源的冲突。

## 评测与应用

### 评测指标与评测方法

根据应用场景的不同，可以分为语言建模、文本分类、条件文本生成、执行类任务以及偏好排序类任务等多个评测指标，如下图：

![评测指标.png](image%2F%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87.png)

- **分类任务 (Classification)**
    
    分类任务要求模型根据给定的输出文本，将其划分到预定义的类别中。最终模型的预测样本可以根据真实类别和预测类别是否一致做二分，分为真正例 (True Postive, TP)、假正例 (False Postive, FP)、真负例 (Ture Negative, TN)、假负例 (False Negative, FN) 四种。
    
    - **精确率**：表示模型预测为正例的样本中真正例的比例
    - **召回率**：表示所有真正为正例的样本中被模型预测出来的比例
    - **F1 分数**：精确率和召回率的调和平均数
- **语言建模任务 (Language Modeling)**
    
    语言模型任务是自然语言处理领域的一项基础任务，旨在根据给定的前文词元来预测后续的词元。
    
    - **困惑度 (Perplexity, PPL)**：通过计算给定文本序列概率的倒数的几何平均，来衡量模型对于语言的建模能力
- **条件文本生成任务 (Conditional Text Generation)**
    
    条件文本生成任务旨在检查模型能否基于输入生成流畅、逻辑连贯且具有实际语义的回复。应用场景覆盖了机器翻译、文本摘要和对话系统等众多场景。
    
    - **BLEU (Bilingual Evaluation Understudy)**：一种在机器翻译领域广泛采用的评估指标，通过计算机器翻译输出 (也称为”候选文本”) 与参考翻译 (也称为”参考文本“) 之间的词汇相似度来评估翻译质量。BLEU 主要计算候选文本与参考文本的 n 元组 (n-gram) 共现频率，评分结果在 [0, 1] 的区间内。
    - **ROUGE-n (Recall-Oriented Understudy for Gisting Evaluation)**：另一种在机器翻译和文本摘要评估中广泛使用的指标。ROUGE 主要侧重于召回率，即强调文本信息的覆盖度和完整性。ROUGE-n 通过计算 n 元组上的召回率来评估候选文本的质量。
    - **ROUGE-L**：ROUGE 的一个重要变种，其中 “L” 代表最长公共子序列 (Longest Common Subsequence, LCS)，用于衡量两个序列的相似性。ROUGE-L 通过寻找候选文本与参考文本之间的最长公共子序列，以 F1 的分数结合了精确率和召回率的信息。
- **问答任务 （Question Answering）**
    
    问答任务主要通过衡量模型给出的答案与真实答案之间的一致性来评估。
    
    - **准确率 (Accuracy)**：旨在计算模型预测正确的样本数占总样本数的比例。但是不同类型的问答任务，判断正确的标注不同：对于数学推理任务，通过参考答案和模型预测答案的等价性判断；对于阅读理解、知识问答等任务，使用精确匹配率和 F1 分数来评估等等。
- **执行类任务**
    
    执行类任务涉及与外部环境交互，评测模型的正确性可以通过外部环境的反馈来判断。
    
    - **成功率 (Success Rate)**：通过衡量模型成功完成任务的次数与任务总数之间的比例，评估模型完成特定任务的能力。
    - **Pass@k**：由 OpenAI 在 HumanEval 数据集中提出的指标。核心思想是计算模型针对单个问题输入生成的 k 个代码输出中，至少有一个代码能通过验证的概率。
- 偏好排序任务
    
    偏好偏序任务是让模型根据某种标准对一组文本或选项进行排序的任务。
    
    - **Elo (Elo Rating System) 评分体系**：广泛应用于各类对弈活动的评价方法。核心思想是通过模型之间的成对比较来动态更新两个模型各自的评分。

接下来介绍一下评测方法及典型的评测工作。

![评测方法.png](image%2F%E8%AF%84%E6%B5%8B%E6%96%B9%E6%B3%95.png)

如上图，根据测评方法主要可以分为三类：

- **基于基准的评测**：这些评测基准通常包含一系列精心设计的任务，且每个任务有充分的测试样本。这种方法的优势在于高度的自动化和可复用性。
- **基于人工的评测**：人工评估在衡量解决实际任务能力方面具有更好的适用性，能够真实反映大语言模型在真实应用场景中的性能表现，并且人工评估还具有高度的灵活性。但相对的人工评估结果受主观因素制约、成本高昂且不易扩展。
- **基于模型的评测**：基于模型的评测方法旨在通过使用其他大模型进行自动化评测，从而降低对于人工参与的依赖度，且该方法在可扩展性和可解释方面较为出色。但是该方法同样面临一系列问题，如位置偏置、冗长偏置和自增强偏置等等。

最后这里给出书中提到的不同能力对应的代表性评测任务与评测数据集，针对不同场景的评测需求可以参考这里：

![数据集.png](image%2F%E6%95%B0%E6%8D%AE%E9%9B%86.png)

### 典型应用场景

#### **传统自然语言领域**

- **序列标注**：这类场景主要包含命名实体识别 (NER) 和 词性标注 (POS) 等，是一种基础的自然语言处理任务。
- **关系抽取**：此类任务关注从非结构化文本数据中自动提取出蕴含的语义关系，比如文本分类或者序列标注任务。
- **文本生成**：这类任务就是现实应用中常见的自然语言处理任务，比如机器翻译和自动摘要。

![传统自然语言领域应用.png](image%2F%E4%BC%A0%E7%BB%9F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8.png)

#### **信息检索任务**

- **大模型提升信息检索**：当前的信息检索通常采用检索-重排序的框架，这部分的应用主要体现在使用小模型辅助稠密检索，比如阿里的 GTE  模型用参数量小的 Qwen 模型做词嵌入 (embedding)，然后通过向量库的混合检索，再输出给重排模型 (reranker) 做重新排序。这部分任务的输出结果是基于知识库的检索信息。
- **检索增强的大模型 (RAG)**：这种场景旨在通过信息检索系统从外部知识库获取相关信息，为大模型提供时效性强、领域相关的外部知识，降低大模型的幻觉并减少错误。例如将上面信息检索的内容给到大模型来完成，特殊情况下还需要对模型进行微调或预训练来提供更加专业的知识内容。

![信息检索任务.png](image%2F%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E4%BB%BB%E5%8A%A1.png)

#### **推荐系统**

- **大模型作为推荐模型**：这种比较好理解，就是直接将大模型作为推荐模型提供服务。实现方式可以通过特定 prompt 提示或者针对性地指令微调。
- **大模型增强的推荐模型**：这种场景大模型主要是用于增强推荐系统的数据输入、语义表示或偏好表示，从而改进目前已有的推荐模型性能。类似于用大模型的语义理解对用户偏好进行分类和标注，从而改进和增强推荐模型。
- **大模型作为推荐模拟器**：大模型被用于设计推荐模拟器，以此来仿真用户在推荐系统中的真实交互行为。这种场景可以为每位用户创建一个基于大语言模型的智能体，以此模拟他们在真实推荐系统中的交互行为。

![推荐系统.png](image%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.png)

#### 多模态

多模态大语言模型（Multimodal Large Language Model, MLLM）主要是指那些
能够处理和整合多种模态信息（比如文本、图像和音频）的大语言模型。例如在大语言模型处理前使用 image-to-text 做图片理解等，在大语言模型处理后使用 text-to-speech 模型输出音频数据等。除此之外，由于视觉模型和语言模型之间存在比较大的语义空间差异，所以需要做对齐训练和指令微调。

![多模态.png](image%2F%E5%A4%9A%E6%A8%A1%E6%80%81.png)

#### 知识图谱

知识图谱 (Knowledge Graph, KG) 中存储了大量的结构化知识信息，常用于知识密集型的任务场景。这里的场景其实主要还是作为大语言模型知识检索的工具。下面提供了两种方式，这两种方式个人认为和目前 RAG 场景中的「多路召回」与「N选1召回」方式类似：第一种是将知识图谱中的知识作为大语言模型的上下文；第二种是作为大语言模型调用工具链中的一环，通过大语言模型自身的动作规划获取知识图谱中的内容。

![知识图谱.png](image%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1.png)

#### 专业领域

涉及专业领域的大语言模型，往往需要经过预训练或者指令微调。下面仅列出相关的模型，感兴趣的可以去详细查查：

![专业模型.png](image%2F%E4%B8%93%E4%B8%9A%E6%A8%A1%E5%9E%8B.png)
